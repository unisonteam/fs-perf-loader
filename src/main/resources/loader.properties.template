#in lower case: reserved words
#in upper case: examples that may be replaced with any value

#it is possible to have many loaders and cleaners in one config
#use same seed to repeat the same load - file names and sizes.
#if seed is not set, it is taken from Unix timestamp
#seed=12345

#optional : set JVM options (default: '-Xmx1G')
#jvmoptions=....
#optional : kerberos keytab and principal
#optional: LD_LIBRARY_PATH env variable for agent (default: /usr/sdp/current/hadoop-client/lib/native)
#libraries=...
#optional: path to directory with Hadoop config files
#confdir=/tmp/hadoop_conf_from_another_cluster
#optional: path to directory with extra Hadoop client jar files (Ozone, etc)
#jarsdir=/tmp/extra_hadoop_jars_from_another_cluster
#kerberos.principal=...
#kerberos.keytab=...

#default: current user
ssh.user=someuser
#connect by key:
#default: $HOME/.ssh/id_rsa
ssh.key=/home/user/.ssh/private_key
#connect by password:
#ssh.password=secret

#optional : prometheus host
#prometheus.address=host:port
#optional : buckets for requests histogram (in millis). see default in the example
#prometheus.buckets=10,20,30,40,50,100,200,300,400,500,1000,2000,5000,10000

#optional: configurations. contains S3 connection properties for S3 or additional HDFS client properties for HDFS
#conf.MYS3.s3.key=accessKeyId
#conf.MYS3.s3.secret=secretAccessKey
#conf.MYS3.s3.uri=http://url...
#it is possible to set S3 endpoint URI in the form of list or template
#conf.S3.s3.uri=http://host{1..5}.domain:port

fsloader.LOAD.hosts=fat01-vm1.unison.team,fat01-vm2.unison.team
fsloader.LOAD.paths=/tmp/path1,/tmp/path2
#alternative way to specify paths (and hosts): 'count:mask'
#fsloader.LOAD.paths=5:/tmp/path%03d
#will be expanded to
#fsloader.LOAD.paths=/tmp/path000,/tmp/path001,/tmp/path002,/tmp/path003,/tmp/path004
#fsloader.LOAD.paths=2:/tmp/path%d/path%d
#will be expanded to
#/tmp/path0/path0,/tmp/path1/path0,/tmp/path0/path1,/tmp/path1/path1
#alternative way to specify paths (and hosts): '{start..end}'
#fsloader.LOAD.paths=/tmp/path{19..21}
#will be expanded to /tmp/path19,/tmp/path20,/tmp/path21
#if this is an S3 configuration, first part of the path will be bucket name, remaining: key prefix. first slash will be ignored. for example:
#paths=/abc/cde/def : bucket: abc, prefix: cde/def
#paths=abc : bucket: abc, prefix: none
#paths=/abc/cde,/def/efg : load will go to bucket abc with prefix cde and to bucket def with prefix efg
#optional: set conf configured above
#fsloader.LOAD.conf=MYS3
#optional: workload json (s3tester format https://github.com/s3tester/s3tester/blob/master/example/workload.json
#or mixed workload   https://github.com/s3tester/s3tester)
#additional option to standard s3tester mixed workload format:
#boolean property for operation
#"randomRead" : true,  (default: false)
#it may be applied to read opertaions - get, head, etc (any operation except 'put' and 'delete')
#if it is used, operation uses not the file created in this sequence but any of files created before.
#to make this option work the rate of 'delete' command should be less than rate of 'put'
#fsloader.LOAD.workload=path/to/workload

#total threads, not for each path
fsloader.LOAD.threads=3
fsloader.LOAD.subdirs.depth=2
fsloader.LOAD.subdirs.width=3
#string format for subdirs. if it is not set - subdirs are just numbers (0/0, etc)
#with the setting below subdirs will be dir000/dir000 , etc
#fsloader.LOAD.subdirs.format=dir%03d

#batches for each path
#number of files in each batch
fsloader.LOAD.batch.file.count=100
#number of batches for each path
fsloader.LOAD.batch.count=100
#loader may fill paths one by one - one batch - one path (type='WINDOW') or
#spread load so every batch contains files for different paths (type='SPREAD').
#default type is 'WINDOW'
#fsloader.LOAD.batch.type=WINDOW

#percentage
fsloader.LOAD.batch.file.sizes=10:10M,5:10K,85:50K
#percentage
fsloader.LOAD.batch.file.suffixes=90:good,10:bad
#optional: write data to a temporary _COPYING_ file and rename if to the final destination at the end (default: false)
#applicable to HDFS only
#fsloader.LOAD.usetmpfile=true
#optional: delay after loading of each file (in millis)
#fsloader.LOAD.delay=100
#optional: delay after each command (in seconds). Default : 0 if prometheus is not enabled and 25 if it is enabled (to separate statistics from different commands)
#fsloader.LOAD.command.delay=30
#optional: number of loads (default: 1 - only one load is done)
#fsloader.LOAD.count=2
#optional: if there are several loads - delay between them in seconds (default: 0)
#fsloader.LOAD.period=60
#optional: fill files with random data (=random) or with a predefined char (=0 for zeroes, =97 for 'a', etc). default: random
#fsloader.LOAD.fill=random

#same as in fsloader
#snapshot.SNAP.hosts=fat01-vm1.unison.team,fat01-vm2.unison.team
#snapshot.SNAP.paths=/tmp/path1
#snapshot.SNAP.threads=3
#snapshot.SNAP.subdirs.depth=2
#snapshot.SNAP.subdirs.width=3
#snapshot.SNAP.subdirs.format=dir%03d
#number of paths in one snapshot batch
#snapshot.SNAP.batch.path.count=10
#optional: in seconds - delay between snapshot batches (default: 60)
#snapshot.SNAP.period=60
#optional: name of snapshot (default: 'snapshot')
#snapshot.SNAP.name=snapshot

#FS cleaner. Runs on one host only
#fscleaner.CLEAN.paths=/tmp/path{19..21}
#fscleaner.CLEAN.host=myhost
#optional: same conf as for loader
#fscleaner.CLEAN.conf=S3
#optional, default - all files
#fscleaner.CLEAN.suffixes=.bak,.old
#optional, default - 8 threads
#fscleaner.CLEAN.threads=10

#directory where jstack and other files will be stored
files.dir=C:/Temp/
jstack.CLIENT.hosts=fat01-vm1.unison.team,fat01-vm2.unison.team
jstack.CLIENT.class=RemoteMain
#file relative to files directory
jstack.CLIENT.file.prefix=client.jstack
#optional - store all jstacks to a single file or separate files (default = true - single file)
#jstack.CLIENT.file.single=true
#optional, default: 30
#jstack.CLIENT.period=15
#compress file (default: false)
#jstack.CLIENT.file.gzip=true
#append output from the new run to old file (default: false - rotate files)
#jstack.CLIENT.file.append=true
#optional - user that should be used to connect to Java process (default: ssh.user)
#jstack.CLIENT.user=serviceuser
#optional - group that should be used to connect to Java process
#jstack.CLIENT.group=servicegroup

#transfer a file from the remote host to local directory set in 'files.dir' property
file.LOG.hosts=fat01-vm1.unison.team,fat01-vm2.unison.team
#optional: path to a file to transfer from the remote host. default: path to agent log
#file.LOG.path=/path/to/file
#user, group, period, file.gzip, file.append, file.prefix are the same as for jstack (file.single is not applicable)

#examples for processes:
#client: class=RemoteMain, user - empty
#OM: class=OzoneManagerStarter, user=ozone
#etc.