#in lower case: reserved words
#in upper case: examples that may be replaced with any value

#it is possible to have many loaders and movers in one config
#use same seed to repeat the same load - file names and sizes.
#if seed is not set, it is taken from Unix timestamp
#seed=12345

#optional : set JVM options (default: '-Xmx1G')
#jvmoptions=....
#optional : kerberos keytab and principal
#kerberos.principal=...
#kerberos.keytab=...

#default: current user
ssh.user=aluht
#connect by key:
#default: $HOME/.ssh/id_rsa
ssh.key=/home/user/.ssh/private_key
#connect by password:
#ssh.password=secret

#optional : prometheus host
#prometheus.address=host:port
#optional : buckets for requests histogram (in millis). see default in the example
#prometheus.buckets=10,20,30,40,50,100,200,300,400,500,1000,2000,5000,10000

#optional: configurations. contains S3 connection properties for S3 or additional HDFS client properties for HDFS
#conf.MYS3.s3.key=accessKeyId
#conf.MYS3.s3.secret=secretAccessKey
#conf.MYS3.s3.uri=http://url...
#it is possible to set S3 endpoint URI in the form of list or template
#conf.S3.s3.uri=http://host{1..5}.domain:port
#in case of multiple URIs it's possible to send requests to one of S3 endpoints randomly (default) or to one by one in cycle
#conf.S3.s3.balance=cycle

fsloader.LOAD.hosts=fat01-vm1.sbrf.ru,fat01-vm2.sbrf.ru
fsloader.LOAD.paths=/tmp/path1,/tmp/path2
#alternative way to specify paths (and hosts): 'count:mask'
#fsloader.LOAD.paths=5:/tmp/path%03d
#will be expanded to
#fsloader.LOAD.paths=/tmp/path000,/tmp/path001,/tmp/path002,/tmp/path003,/tmp/path004
#fsloader.LOAD.paths=2:/tmp/path%d/path%d
#will be expanded to
#/tmp/path0/path0,/tmp/path1/path0,/tmp/path0/path1,/tmp/path1/path1
#alternative way to specify paths (and hosts): '{start..end}'
#fsloader.LOAD.paths=/tmp/path{19..21}
#will be expanded to /tmp/path19,/tmp/path20,/tmp/path21
#if this is an S3 configuration, first part of the path will be bucket name, remaining: key prefix. first slash will be ignored. for example:
#paths=/abc/cde/def : bucket: abc, prefix: cde/def
#paths=abc : bucket: abc, prefix: none
#paths=/abc/cde,/def/efg : load will go to bucket abc with prefix cde and to bucket def with prefix efg
#optional: set conf configured above
#fsloader.LOAD.conf=MYS3
#optional: workload json (s3tester format https://github.com/s3tester/s3tester/blob/master/example/workload.json)
#fsloader.LOAD.workload=path/to/workload

#total threads, not for each path
fsloader.LOAD.threads=3
fsloader.LOAD.subdirs.depth=2
fsloader.LOAD.subdirs.width=3
#string format for subdirs. if it is not set - subdirs are just numbers (0/0, etc)
#with the setting below subdirs will be dir000/dir000 , etc
#fsloader.LOAD.subdirs.format=dir%03d

#batches for each path
#number of files in each batch
fsloader.LOAD.batch.file.count=100
#number of batches for each path
fsloader.LOAD.batch.count=100
#loader may fill paths one by one - one batch - one path (type='WINDOW') or
#spread load so every batch contains files for different paths (type='SPREAD').
#default type is 'WINDOW'
#fsloader.LOAD.batch.type=WINDOW

#percentage
fsloader.LOAD.batch.file.sizes=10:10M,5:10K,85:50K
#percentage
fsloader.LOAD.batch.file.suffixes=90:good,10:bad
#optional: write data to a temporary _COPYING_ file and rename if to the final destination at the end (default: true)
#applicable to HDFS only
#fsloader.LOAD.usetmpfile=true
#optional: delay after loading of each file (in millis)
#fsloader.LOAD.delay=100
#optional: number of loads (default: 1 - only one load is done)
#fsloader.LOAD.count=2
#optional: if there are several loads - delay between them in seconds (default: 0)
#fsloader.LOAD.period=60
#optional: fill files with random data (=random) or with a predefined char (=0 for zeroes, =97 for 'a', etc). default: random
#fsloader.LOAD.fill=random

#FS cleaner. Runs on one host only
#fscleaner.CLEAN.paths=/tmp/path{19..21}
#fscleaner.CLEAN.host=myhost
#optional: same conf as for loader
#fscleaner.CLEAN.conf=S3
#optional, default - all files
#fscleaner.CLEAN.suffixes=.bak,.old
#optional, default - 8 threads
#fscleaner.CLEAN.threads=10
